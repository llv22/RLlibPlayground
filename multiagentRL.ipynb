{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent RL\n",
    "\n",
    "Don't find **MultiAgentTrafficEnv**, **car_obs_space**, **car_act_space** and **tl_obs_space**, **tl_act_space**\n",
    "**Status** : Not working now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "# check in https://github.com/ray-project/ray/blob/master/python/ray/rllib/test/test_nested_spaces.py\n",
    "from ray.rllib.agents import pg\n",
    "from ray.rllib.agents.pg.pg_policy_graph import PGPolicyGraph\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: using a multi-agent env\n",
    "env = MultiAgentTrafficEnv(num_cars=20, num_traffic_lights=5)\n",
    "\n",
    "# Observations are a dict mapping agent names to their obs. Not all agents\n",
    "# may be present in the dict in each time step.\n",
    "print(env.reset())\n",
    "# {\n",
    "#     \"car_1\": [[...]],\n",
    "#     \"car_2\": [[...]],\n",
    "#     \"traffic_light_1\": [[...]],\n",
    "# }\n",
    "\n",
    "# Actions should be provided for each agent that returned an observation.\n",
    "new_obs, rewards, dones, infos = env.step(actions={\"car_1\": ..., \"car_2\": ...})\n",
    "\n",
    "# Similarly, new_obs, rewards, dones, etc. also become dicts\n",
    "print(rewards)\n",
    "# {\"car_1\": 3, \"car_2\": -1, \"traffic_light_1\": 0}\n",
    "\n",
    "# Individual agents can early exit; env is done when \"__all__\" = True\n",
    "print(dones)\n",
    "# {\"car_2\": True, \"__all__\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rise.cs.berkeley.edu/blog/scaling-multi-agent-rl-with-rllib/\n",
    "# https://ray.readthedocs.io/en/latest/rllib-env.html?highlight=%27PGAgent%27\n",
    "# https://github.com/ray-project/ray/blob/master/python/ray/rllib/examples/multiagent_cartpole.py\n",
    "trainer = pg.PGAgent(env=\"my_multiagent_env\", config={\n",
    "    \"multiagent\": {\n",
    "        \"policy_graphs\": {\n",
    "            \"car1\": (PGPolicyGraph, car_obs_space, car_act_space, {\"gamma\": 0.85}),\n",
    "            \"car2\": (PGPolicyGraph, car_obs_space, car_act_space, {\"gamma\": 0.99}),\n",
    "            \"traffic_light\": (PGPolicyGraph, tl_obs_space, tl_act_space, {}),\n",
    "        },\n",
    "        \"policy_mapping_fn\": {\n",
    "            lambda agent_id:\n",
    "                \"traffic_light\"  # Traffic lights are always controlled by this policy\n",
    "                if agent_id.startswith(\"traffic_light_\")\n",
    "                else random.choice([\"car1\", \"car2\"])  # Randomly choose from car policies\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(trainer.train())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
